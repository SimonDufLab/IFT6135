{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q2_VAE.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "G954m99gJU-r",
        "colab_type": "code",
        "outputId": "98efd2c1-e51f-4817-e8aa-3904883fb9a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torchvision.datasets import utils\n",
        "\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    use_cuda = True\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    use_cuda = False\n",
        "    \n",
        "print(device)\n",
        "\n",
        "print(f\"Your version of Pytorch is {torch.__version__}. You should use a version >0.4.\") #Check"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Your version of Pytorch is 1.0.1.post2. You should use a version >0.4.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DYd_97AIMy_g",
        "colab_type": "code",
        "outputId": "a1ed7bec-5855-437a-bd8d-7f6e0c3b13e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        }
      },
      "cell_type": "code",
      "source": [
        "### Importing the data with the given loader\n",
        "def get_data_loader(dataset_location, batch_size):\n",
        "    URL = \"http://www.cs.toronto.edu/~larocheh/public/datasets/binarized_mnist/\"\n",
        "    # start processing\n",
        "    def lines_to_np_array(lines):\n",
        "        return np.array([[int(i) for i in line.split()] for line in lines])\n",
        "    splitdata = []\n",
        "    for splitname in [\"train\", \"valid\", \"test\"]:\n",
        "        filename = \"binarized_mnist_%s.amat\" % splitname\n",
        "        filepath = os.path.join(dataset_location, filename)\n",
        "        utils.download_url(URL + filename, dataset_location)\n",
        "        with open(filepath) as f:\n",
        "            lines = f.readlines()\n",
        "        x = lines_to_np_array(lines).astype('float32')\n",
        "        x = x.reshape(x.shape[0], 1, 28, 28)\n",
        "        # pytorch data loader\n",
        "        dataset = torch.utils.data.TensorDataset(torch.from_numpy(x))\n",
        "        print(splitname+\" : \"+str(x.shape))\n",
        "        dataset_loader = torch.utils.data.DataLoader(x, batch_size=batch_size, shuffle=splitname == \"train\")\n",
        "        splitdata.append(dataset_loader)\n",
        "    return splitdata\n",
        "  \n",
        "train, valid, test = get_data_loader(\"binarized_mnist\", 64)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/78400000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://www.cs.toronto.edu/~larocheh/public/datasets/binarized_mnist/binarized_mnist_train.amat to binarized_mnist/binarized_mnist_train.amat\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "78405632it [00:04, 16697557.34it/s]                              \n",
            "  0%|          | 16384/15680000 [00:00<01:37, 160597.80it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train : (50000, 1, 28, 28)\n",
            "Downloading http://www.cs.toronto.edu/~larocheh/public/datasets/binarized_mnist/binarized_mnist_valid.amat to binarized_mnist/binarized_mnist_valid.amat\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15687680it [00:01, 9217571.81it/s]                              \n",
            "  0%|          | 16384/15680000 [00:00<01:39, 157575.53it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "valid : (10000, 1, 28, 28)\n",
            "Downloading http://www.cs.toronto.edu/~larocheh/public/datasets/binarized_mnist/binarized_mnist_test.amat to binarized_mnist/binarized_mnist_test.amat\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "15687680it [00:03, 4601117.93it/s]                              \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test : (10000, 1, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NJvK98SFKyJZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class VAE_100dim(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VAE_100dim, self).__init__()\n",
        "        \n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3),\n",
        "            nn.ELU(),\n",
        "            nn.AvgPool2d(2, stride=2),\n",
        "            nn.Conv2d(32, 64, 3),\n",
        "            nn.ELU(),\n",
        "            nn.AvgPool2d(2, stride=2),\n",
        "            nn.Conv2d(64, 256, 5),\n",
        "            nn.ELU())\n",
        "        self.h1=nn.Linear(256,200)                   #output mu and logvar\n",
        "        \n",
        "        self.h2=nn.Linear(100,256)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ELU(),\n",
        "            nn.Conv2d(256, 64, 5, padding=4),\n",
        "            nn.ELU(),\n",
        "            nn.UpsamplingBilinear2d(scale_factor=2),\n",
        "            nn.Conv2d(64, 32, 3, padding=2),\n",
        "            nn.ELU(),\n",
        "            nn.UpsamplingBilinear2d(scale_factor=2),\n",
        "            nn.Conv2d(32, 16, 3, padding=2),\n",
        "            nn.ELU(),\n",
        "            nn.Conv2d(16, 1, 3, padding=2))\n",
        "        \n",
        "    def forward(self,x):\n",
        "      x = self.encoder(x)\n",
        "      q = self.h1(x.view(-1,256))\n",
        "      mu,logvar = torch.split(q,100,dim=1)\n",
        "      \n",
        "      std = (logvar*0.5).exp()\n",
        "      eps = torch.randn_like(mu).to(device)\n",
        "      z = mu + (eps*std)\n",
        "      \n",
        "      x_ = self.h2(z)\n",
        "      x_ = self.decoder(x_.view(-1,256,1,1))\n",
        "      \n",
        "      return x_, mu, logvar\n",
        "      \n",
        "    def generate_new_data(self,z):\n",
        "      \"\"\" The following function take the z sampled from distribution and generate new examples\n",
        "              \n",
        "      \"\"\"\n",
        "      with torch.no_grad():    #no gradients to accumulate here, i.e. faster\n",
        "        return self.decoder(self.h2(z).view(-1,256,1,1))\n",
        "      \n",
        "def loss_function(x_,x,mu,logvar):\n",
        "  \"\"\" The Loss is a combination of the reconstruction loss (here binary cross entropy)\n",
        "      and the KLD.\n",
        "  \"\"\"\n",
        "  BCE = F.binary_cross_entropy_with_logits(x_.view(-1, 784), x.view(-1, 784), reduction='sum')\n",
        "  \n",
        "  KLD = -0.5 * torch.sum(1 + logvar - mu**2 - logvar.exp())\n",
        "  \n",
        "  return (BCE+KLD)/x.shape[0]\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jgUgD72YYnid",
        "colab_type": "code",
        "outputId": "a1c77ee9-9ec0-49f6-f037-30b09a621089",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "clf = VAE_100dim()\n",
        "cuda_available = torch.cuda.is_available()\n",
        "print(cuda_available)\n",
        "if cuda_available:\n",
        "    clf = clf.cuda()\n",
        "optimizer = optim.Adam(clf.parameters(),lr=3e-4)\n",
        "criterion = loss_function"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wcL9J-g6Y0N6",
        "colab_type": "code",
        "outputId": "f12166a2-d62b-43dc-fd1f-853e1177ca74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3823
        }
      },
      "cell_type": "code",
      "source": [
        "## Let's train for 20 epochs\n",
        "\n",
        "tracking_loss=[]\n",
        "for epoch in range(20):\n",
        "  \n",
        "  training_loss=0\n",
        "  for batch_idx, x in enumerate(train):\n",
        "    x = x.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    #Proper training\n",
        "    x_, mu, logvar = clf(x)\n",
        "\n",
        "    loss = criterion(x_, x, mu, logvar)\n",
        "    loss.backward()\n",
        "    training_loss += loss.item()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if (batch_idx+1) % 78 == 0:\n",
        "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tELBO: {:.6f}'.format(\n",
        "            epoch, batch_idx * len(x), len(train.dataset),\n",
        "            100. * batch_idx / len(train),\n",
        "            -1*loss.item()))\n",
        "  train_batches = batch_idx\n",
        "\n",
        "  tracking_loss.append(training_loss)\n",
        "  clf.eval()\n",
        "  valid_loss=0\n",
        "  for batch_idx, x in enumerate(valid):\n",
        "    x = x.to(device)\n",
        "    x_, mu, logvar = clf(x)\n",
        "    loss = criterion(x_, x, mu, logvar)\n",
        "    valid_loss += loss.item()\n",
        "  valid_batches = batch_idx\n",
        "  clf.train()\n",
        "  \n",
        "  print('====> Epoch: {} Avg ELBO on Train: {:.4f} || Avg ELBO on Valid: {:.4f}'.format(\n",
        "      epoch, -1*training_loss/train_batches, -1*valid_loss/valid_batches ))\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.UpsamplingBilinear2d is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [4928/50000 (10%)]\tELBO: -91.831421\n",
            "Train Epoch: 0 [9920/50000 (20%)]\tELBO: -103.357056\n",
            "Train Epoch: 0 [14912/50000 (30%)]\tELBO: -98.762108\n",
            "Train Epoch: 0 [19904/50000 (40%)]\tELBO: -101.504089\n",
            "Train Epoch: 0 [24896/50000 (50%)]\tELBO: -96.403015\n",
            "Train Epoch: 0 [29888/50000 (60%)]\tELBO: -95.264130\n",
            "Train Epoch: 0 [34880/50000 (70%)]\tELBO: -98.730591\n",
            "Train Epoch: 0 [39872/50000 (80%)]\tELBO: -97.590103\n",
            "Train Epoch: 0 [44864/50000 (90%)]\tELBO: -96.061676\n",
            "Train Epoch: 0 [49856/50000 (100%)]\tELBO: -103.579544\n",
            "====> Epoch: 0 Avg ELBO on Train: -99.0257 || Avg ELBO on Valid: -99.5495\n",
            "Train Epoch: 1 [4928/50000 (10%)]\tELBO: -100.176613\n",
            "Train Epoch: 1 [9920/50000 (20%)]\tELBO: -101.914055\n",
            "Train Epoch: 1 [14912/50000 (30%)]\tELBO: -98.133759\n",
            "Train Epoch: 1 [19904/50000 (40%)]\tELBO: -96.813828\n",
            "Train Epoch: 1 [24896/50000 (50%)]\tELBO: -94.817352\n",
            "Train Epoch: 1 [29888/50000 (60%)]\tELBO: -100.629776\n",
            "Train Epoch: 1 [34880/50000 (70%)]\tELBO: -98.279404\n",
            "Train Epoch: 1 [39872/50000 (80%)]\tELBO: -99.414299\n",
            "Train Epoch: 1 [44864/50000 (90%)]\tELBO: -96.611542\n",
            "Train Epoch: 1 [49856/50000 (100%)]\tELBO: -103.232460\n",
            "====> Epoch: 1 Avg ELBO on Train: -98.1449 || Avg ELBO on Valid: -98.5810\n",
            "Train Epoch: 2 [4928/50000 (10%)]\tELBO: -95.335968\n",
            "Train Epoch: 2 [9920/50000 (20%)]\tELBO: -107.272354\n",
            "Train Epoch: 2 [14912/50000 (30%)]\tELBO: -102.116776\n",
            "Train Epoch: 2 [19904/50000 (40%)]\tELBO: -96.842377\n",
            "Train Epoch: 2 [24896/50000 (50%)]\tELBO: -100.142586\n",
            "Train Epoch: 2 [29888/50000 (60%)]\tELBO: -101.761917\n",
            "Train Epoch: 2 [34880/50000 (70%)]\tELBO: -100.976303\n",
            "Train Epoch: 2 [39872/50000 (80%)]\tELBO: -101.582382\n",
            "Train Epoch: 2 [44864/50000 (90%)]\tELBO: -98.794052\n",
            "Train Epoch: 2 [49856/50000 (100%)]\tELBO: -98.821671\n",
            "====> Epoch: 2 Avg ELBO on Train: -97.4599 || Avg ELBO on Valid: -98.2793\n",
            "Train Epoch: 3 [4928/50000 (10%)]\tELBO: -94.562218\n",
            "Train Epoch: 3 [9920/50000 (20%)]\tELBO: -99.012360\n",
            "Train Epoch: 3 [14912/50000 (30%)]\tELBO: -99.852119\n",
            "Train Epoch: 3 [19904/50000 (40%)]\tELBO: -89.949005\n",
            "Train Epoch: 3 [24896/50000 (50%)]\tELBO: -99.605339\n",
            "Train Epoch: 3 [29888/50000 (60%)]\tELBO: -99.733063\n",
            "Train Epoch: 3 [34880/50000 (70%)]\tELBO: -101.276611\n",
            "Train Epoch: 3 [39872/50000 (80%)]\tELBO: -93.404350\n",
            "Train Epoch: 3 [44864/50000 (90%)]\tELBO: -95.600601\n",
            "Train Epoch: 3 [49856/50000 (100%)]\tELBO: -99.995743\n",
            "====> Epoch: 3 Avg ELBO on Train: -96.8526 || Avg ELBO on Valid: -97.4045\n",
            "Train Epoch: 4 [4928/50000 (10%)]\tELBO: -94.814919\n",
            "Train Epoch: 4 [9920/50000 (20%)]\tELBO: -95.131363\n",
            "Train Epoch: 4 [14912/50000 (30%)]\tELBO: -92.761002\n",
            "Train Epoch: 4 [19904/50000 (40%)]\tELBO: -89.674927\n",
            "Train Epoch: 4 [24896/50000 (50%)]\tELBO: -96.729492\n",
            "Train Epoch: 4 [29888/50000 (60%)]\tELBO: -96.251968\n",
            "Train Epoch: 4 [34880/50000 (70%)]\tELBO: -95.210632\n",
            "Train Epoch: 4 [39872/50000 (80%)]\tELBO: -93.408600\n",
            "Train Epoch: 4 [44864/50000 (90%)]\tELBO: -91.706940\n",
            "Train Epoch: 4 [49856/50000 (100%)]\tELBO: -94.281204\n",
            "====> Epoch: 4 Avg ELBO on Train: -96.3483 || Avg ELBO on Valid: -96.8523\n",
            "Train Epoch: 5 [4928/50000 (10%)]\tELBO: -98.147644\n",
            "Train Epoch: 5 [9920/50000 (20%)]\tELBO: -100.030533\n",
            "Train Epoch: 5 [14912/50000 (30%)]\tELBO: -93.452072\n",
            "Train Epoch: 5 [19904/50000 (40%)]\tELBO: -97.272591\n",
            "Train Epoch: 5 [24896/50000 (50%)]\tELBO: -91.509064\n",
            "Train Epoch: 5 [29888/50000 (60%)]\tELBO: -93.497559\n",
            "Train Epoch: 5 [34880/50000 (70%)]\tELBO: -89.437767\n",
            "Train Epoch: 5 [39872/50000 (80%)]\tELBO: -97.926239\n",
            "Train Epoch: 5 [44864/50000 (90%)]\tELBO: -101.423965\n",
            "Train Epoch: 5 [49856/50000 (100%)]\tELBO: -91.142609\n",
            "====> Epoch: 5 Avg ELBO on Train: -95.9207 || Avg ELBO on Valid: -96.7931\n",
            "Train Epoch: 6 [4928/50000 (10%)]\tELBO: -95.911362\n",
            "Train Epoch: 6 [9920/50000 (20%)]\tELBO: -96.629349\n",
            "Train Epoch: 6 [14912/50000 (30%)]\tELBO: -96.498322\n",
            "Train Epoch: 6 [19904/50000 (40%)]\tELBO: -94.457375\n",
            "Train Epoch: 6 [24896/50000 (50%)]\tELBO: -95.334213\n",
            "Train Epoch: 6 [29888/50000 (60%)]\tELBO: -93.920502\n",
            "Train Epoch: 6 [34880/50000 (70%)]\tELBO: -98.404854\n",
            "Train Epoch: 6 [39872/50000 (80%)]\tELBO: -93.174789\n",
            "Train Epoch: 6 [44864/50000 (90%)]\tELBO: -98.081711\n",
            "Train Epoch: 6 [49856/50000 (100%)]\tELBO: -99.087471\n",
            "====> Epoch: 6 Avg ELBO on Train: -95.5146 || Avg ELBO on Valid: -96.0883\n",
            "Train Epoch: 7 [4928/50000 (10%)]\tELBO: -96.159378\n",
            "Train Epoch: 7 [9920/50000 (20%)]\tELBO: -93.679291\n",
            "Train Epoch: 7 [14912/50000 (30%)]\tELBO: -93.425140\n",
            "Train Epoch: 7 [19904/50000 (40%)]\tELBO: -97.890617\n",
            "Train Epoch: 7 [24896/50000 (50%)]\tELBO: -97.218262\n",
            "Train Epoch: 7 [29888/50000 (60%)]\tELBO: -96.872696\n",
            "Train Epoch: 7 [34880/50000 (70%)]\tELBO: -86.979889\n",
            "Train Epoch: 7 [39872/50000 (80%)]\tELBO: -92.985413\n",
            "Train Epoch: 7 [44864/50000 (90%)]\tELBO: -96.955528\n",
            "Train Epoch: 7 [49856/50000 (100%)]\tELBO: -95.914261\n",
            "====> Epoch: 7 Avg ELBO on Train: -95.0871 || Avg ELBO on Valid: -96.3194\n",
            "Train Epoch: 8 [4928/50000 (10%)]\tELBO: -94.471069\n",
            "Train Epoch: 8 [9920/50000 (20%)]\tELBO: -94.892693\n",
            "Train Epoch: 8 [14912/50000 (30%)]\tELBO: -95.872833\n",
            "Train Epoch: 8 [19904/50000 (40%)]\tELBO: -99.268425\n",
            "Train Epoch: 8 [24896/50000 (50%)]\tELBO: -93.137871\n",
            "Train Epoch: 8 [29888/50000 (60%)]\tELBO: -90.939301\n",
            "Train Epoch: 8 [34880/50000 (70%)]\tELBO: -90.388390\n",
            "Train Epoch: 8 [39872/50000 (80%)]\tELBO: -90.019295\n",
            "Train Epoch: 8 [44864/50000 (90%)]\tELBO: -90.640549\n",
            "Train Epoch: 8 [49856/50000 (100%)]\tELBO: -92.193657\n",
            "====> Epoch: 8 Avg ELBO on Train: -94.7804 || Avg ELBO on Valid: -95.5461\n",
            "Train Epoch: 9 [4928/50000 (10%)]\tELBO: -94.557159\n",
            "Train Epoch: 9 [9920/50000 (20%)]\tELBO: -99.499855\n",
            "Train Epoch: 9 [14912/50000 (30%)]\tELBO: -92.473282\n",
            "Train Epoch: 9 [19904/50000 (40%)]\tELBO: -92.918823\n",
            "Train Epoch: 9 [24896/50000 (50%)]\tELBO: -93.362068\n",
            "Train Epoch: 9 [29888/50000 (60%)]\tELBO: -94.190704\n",
            "Train Epoch: 9 [34880/50000 (70%)]\tELBO: -94.632210\n",
            "Train Epoch: 9 [39872/50000 (80%)]\tELBO: -97.068329\n",
            "Train Epoch: 9 [44864/50000 (90%)]\tELBO: -95.715088\n",
            "Train Epoch: 9 [49856/50000 (100%)]\tELBO: -90.955193\n",
            "====> Epoch: 9 Avg ELBO on Train: -94.4900 || Avg ELBO on Valid: -95.4545\n",
            "Train Epoch: 10 [4928/50000 (10%)]\tELBO: -94.276741\n",
            "Train Epoch: 10 [9920/50000 (20%)]\tELBO: -90.930161\n",
            "Train Epoch: 10 [14912/50000 (30%)]\tELBO: -91.769699\n",
            "Train Epoch: 10 [19904/50000 (40%)]\tELBO: -93.160614\n",
            "Train Epoch: 10 [24896/50000 (50%)]\tELBO: -97.410431\n",
            "Train Epoch: 10 [29888/50000 (60%)]\tELBO: -93.355339\n",
            "Train Epoch: 10 [34880/50000 (70%)]\tELBO: -86.755760\n",
            "Train Epoch: 10 [39872/50000 (80%)]\tELBO: -96.659958\n",
            "Train Epoch: 10 [44864/50000 (90%)]\tELBO: -97.338959\n",
            "Train Epoch: 10 [49856/50000 (100%)]\tELBO: -95.694687\n",
            "====> Epoch: 10 Avg ELBO on Train: -94.2002 || Avg ELBO on Valid: -94.9767\n",
            "Train Epoch: 11 [4928/50000 (10%)]\tELBO: -92.422653\n",
            "Train Epoch: 11 [9920/50000 (20%)]\tELBO: -87.289078\n",
            "Train Epoch: 11 [14912/50000 (30%)]\tELBO: -89.433121\n",
            "Train Epoch: 11 [19904/50000 (40%)]\tELBO: -96.532074\n",
            "Train Epoch: 11 [24896/50000 (50%)]\tELBO: -91.649078\n",
            "Train Epoch: 11 [29888/50000 (60%)]\tELBO: -91.963837\n",
            "Train Epoch: 11 [34880/50000 (70%)]\tELBO: -99.538086\n",
            "Train Epoch: 11 [39872/50000 (80%)]\tELBO: -93.408890\n",
            "Train Epoch: 11 [44864/50000 (90%)]\tELBO: -94.526520\n",
            "Train Epoch: 11 [49856/50000 (100%)]\tELBO: -95.839752\n",
            "====> Epoch: 11 Avg ELBO on Train: -93.9941 || Avg ELBO on Valid: -94.7306\n",
            "Train Epoch: 12 [4928/50000 (10%)]\tELBO: -97.846466\n",
            "Train Epoch: 12 [9920/50000 (20%)]\tELBO: -91.783989\n",
            "Train Epoch: 12 [14912/50000 (30%)]\tELBO: -91.298355\n",
            "Train Epoch: 12 [19904/50000 (40%)]\tELBO: -91.331070\n",
            "Train Epoch: 12 [24896/50000 (50%)]\tELBO: -95.428879\n",
            "Train Epoch: 12 [29888/50000 (60%)]\tELBO: -92.375198\n",
            "Train Epoch: 12 [34880/50000 (70%)]\tELBO: -93.100937\n",
            "Train Epoch: 12 [39872/50000 (80%)]\tELBO: -95.143730\n",
            "Train Epoch: 12 [44864/50000 (90%)]\tELBO: -91.888504\n",
            "Train Epoch: 12 [49856/50000 (100%)]\tELBO: -89.211975\n",
            "====> Epoch: 12 Avg ELBO on Train: -93.6922 || Avg ELBO on Valid: -94.8787\n",
            "Train Epoch: 13 [4928/50000 (10%)]\tELBO: -95.904198\n",
            "Train Epoch: 13 [9920/50000 (20%)]\tELBO: -91.597656\n",
            "Train Epoch: 13 [14912/50000 (30%)]\tELBO: -95.234360\n",
            "Train Epoch: 13 [19904/50000 (40%)]\tELBO: -93.675156\n",
            "Train Epoch: 13 [24896/50000 (50%)]\tELBO: -90.248154\n",
            "Train Epoch: 13 [29888/50000 (60%)]\tELBO: -87.377266\n",
            "Train Epoch: 13 [34880/50000 (70%)]\tELBO: -88.314758\n",
            "Train Epoch: 13 [39872/50000 (80%)]\tELBO: -93.807014\n",
            "Train Epoch: 13 [44864/50000 (90%)]\tELBO: -90.479095\n",
            "Train Epoch: 13 [49856/50000 (100%)]\tELBO: -87.836075\n",
            "====> Epoch: 13 Avg ELBO on Train: -93.5771 || Avg ELBO on Valid: -94.4348\n",
            "Train Epoch: 14 [4928/50000 (10%)]\tELBO: -90.155334\n",
            "Train Epoch: 14 [9920/50000 (20%)]\tELBO: -96.835815\n",
            "Train Epoch: 14 [14912/50000 (30%)]\tELBO: -93.727547\n",
            "Train Epoch: 14 [19904/50000 (40%)]\tELBO: -95.935936\n",
            "Train Epoch: 14 [24896/50000 (50%)]\tELBO: -88.866043\n",
            "Train Epoch: 14 [29888/50000 (60%)]\tELBO: -93.112251\n",
            "Train Epoch: 14 [34880/50000 (70%)]\tELBO: -93.079361\n",
            "Train Epoch: 14 [39872/50000 (80%)]\tELBO: -93.912048\n",
            "Train Epoch: 14 [44864/50000 (90%)]\tELBO: -93.206696\n",
            "Train Epoch: 14 [49856/50000 (100%)]\tELBO: -93.823463\n",
            "====> Epoch: 14 Avg ELBO on Train: -93.3704 || Avg ELBO on Valid: -94.3528\n",
            "Train Epoch: 15 [4928/50000 (10%)]\tELBO: -93.068138\n",
            "Train Epoch: 15 [9920/50000 (20%)]\tELBO: -92.791504\n",
            "Train Epoch: 15 [14912/50000 (30%)]\tELBO: -93.853432\n",
            "Train Epoch: 15 [19904/50000 (40%)]\tELBO: -92.698486\n",
            "Train Epoch: 15 [24896/50000 (50%)]\tELBO: -94.628479\n",
            "Train Epoch: 15 [29888/50000 (60%)]\tELBO: -91.316902\n",
            "Train Epoch: 15 [34880/50000 (70%)]\tELBO: -91.923515\n",
            "Train Epoch: 15 [39872/50000 (80%)]\tELBO: -92.126503\n",
            "Train Epoch: 15 [44864/50000 (90%)]\tELBO: -94.828506\n",
            "Train Epoch: 15 [49856/50000 (100%)]\tELBO: -87.335411\n",
            "====> Epoch: 15 Avg ELBO on Train: -93.2031 || Avg ELBO on Valid: -94.2456\n",
            "Train Epoch: 16 [4928/50000 (10%)]\tELBO: -93.018547\n",
            "Train Epoch: 16 [9920/50000 (20%)]\tELBO: -99.017853\n",
            "Train Epoch: 16 [14912/50000 (30%)]\tELBO: -94.488022\n",
            "Train Epoch: 16 [19904/50000 (40%)]\tELBO: -96.114113\n",
            "Train Epoch: 16 [24896/50000 (50%)]\tELBO: -94.921341\n",
            "Train Epoch: 16 [29888/50000 (60%)]\tELBO: -91.366142\n",
            "Train Epoch: 16 [34880/50000 (70%)]\tELBO: -93.744949\n",
            "Train Epoch: 16 [39872/50000 (80%)]\tELBO: -89.012856\n",
            "Train Epoch: 16 [44864/50000 (90%)]\tELBO: -92.122070\n",
            "Train Epoch: 16 [49856/50000 (100%)]\tELBO: -94.622231\n",
            "====> Epoch: 16 Avg ELBO on Train: -93.0004 || Avg ELBO on Valid: -93.9595\n",
            "Train Epoch: 17 [4928/50000 (10%)]\tELBO: -90.059189\n",
            "Train Epoch: 17 [9920/50000 (20%)]\tELBO: -96.141930\n",
            "Train Epoch: 17 [14912/50000 (30%)]\tELBO: -90.699837\n",
            "Train Epoch: 17 [19904/50000 (40%)]\tELBO: -86.556442\n",
            "Train Epoch: 17 [24896/50000 (50%)]\tELBO: -101.084785\n",
            "Train Epoch: 17 [29888/50000 (60%)]\tELBO: -96.482147\n",
            "Train Epoch: 17 [34880/50000 (70%)]\tELBO: -89.383636\n",
            "Train Epoch: 17 [39872/50000 (80%)]\tELBO: -94.693016\n",
            "Train Epoch: 17 [44864/50000 (90%)]\tELBO: -92.321716\n",
            "Train Epoch: 17 [49856/50000 (100%)]\tELBO: -90.362305\n",
            "====> Epoch: 17 Avg ELBO on Train: -92.7769 || Avg ELBO on Valid: -93.8970\n",
            "Train Epoch: 18 [4928/50000 (10%)]\tELBO: -96.191414\n",
            "Train Epoch: 18 [9920/50000 (20%)]\tELBO: -90.737755\n",
            "Train Epoch: 18 [14912/50000 (30%)]\tELBO: -89.538918\n",
            "Train Epoch: 18 [19904/50000 (40%)]\tELBO: -92.161293\n",
            "Train Epoch: 18 [24896/50000 (50%)]\tELBO: -89.409004\n",
            "Train Epoch: 18 [29888/50000 (60%)]\tELBO: -86.285912\n",
            "Train Epoch: 18 [34880/50000 (70%)]\tELBO: -89.748375\n",
            "Train Epoch: 18 [39872/50000 (80%)]\tELBO: -90.658112\n",
            "Train Epoch: 18 [44864/50000 (90%)]\tELBO: -95.648491\n",
            "Train Epoch: 18 [49856/50000 (100%)]\tELBO: -94.499039\n",
            "====> Epoch: 18 Avg ELBO on Train: -92.6511 || Avg ELBO on Valid: -93.8977\n",
            "Train Epoch: 19 [4928/50000 (10%)]\tELBO: -94.800491\n",
            "Train Epoch: 19 [9920/50000 (20%)]\tELBO: -94.688019\n",
            "Train Epoch: 19 [14912/50000 (30%)]\tELBO: -88.046333\n",
            "Train Epoch: 19 [19904/50000 (40%)]\tELBO: -94.752975\n",
            "Train Epoch: 19 [24896/50000 (50%)]\tELBO: -93.594803\n",
            "Train Epoch: 19 [29888/50000 (60%)]\tELBO: -92.902313\n",
            "Train Epoch: 19 [34880/50000 (70%)]\tELBO: -89.582047\n",
            "Train Epoch: 19 [39872/50000 (80%)]\tELBO: -94.657654\n",
            "Train Epoch: 19 [44864/50000 (90%)]\tELBO: -90.966080\n",
            "Train Epoch: 19 [49856/50000 (100%)]\tELBO: -91.256493\n",
            "====> Epoch: 19 Avg ELBO on Train: -92.5147 || Avg ELBO on Valid: -93.5226\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-3k5MLUgNo3S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "torch.save(clf.state_dict(),\"../Vae.pt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fb7jC3bpNp9J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Loading the trained model to recuparate param:\n",
        "clf.load_state_dict(torch.load(\"../content/2_Vae.pt\"))\n",
        "clf.eval()\n",
        "\n",
        "\n",
        "#Evaluating log-likelihood VAE\n",
        "\n",
        "## Here, we want to compute log(p(x|z)), log(p(z)) and log(q(z|x)) in order to use the LogSumExp trick\n",
        "\n",
        "##### Note: We have log(p(x|z)) from BCE calculation. We need to compute log(p(z)/q(z|x)), modelised with multivariate gaussians.\n",
        "##### Some terms canceled out, such as sqrt(2*pi)^100\n",
        "\n",
        "#Given x and z:\n",
        "def logP_xi(AE,x):\n",
        "  \"\"\" Calculate the log-likelihood for a given batch of data\n",
        "  \n",
        "      Call it M time (once per batch) to get\n",
        "      (log p(x_1),...,log(px_M)) estimates of size (M,)\n",
        "  \"\"\"\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    x_, mu, logvar = AE(x)\n",
        "    \n",
        "    ### Let's sample the z_i from q_phi:\n",
        "    mu, logvar = mu.repeat(1,200), logvar.repeat(1,200)        #K=200 samples per x_i\n",
        "    z = torch.normal(mu,(logvar*0.5).exp())                  #z_i^k sampled from q(z|x_i)!!\n",
        "    z = z.reshape(-1,100)\n",
        "    #print(z.size())\n",
        "    x_= AE.generate_new_data(z)                        #getting g(z) for log(p(x|z))\n",
        "    x_ = x_.reshape(x.shape[0],200,784)\n",
        "    \n",
        "    x = x.view(-1,784).repeat(1,200)                   #Need to compare each x_i to the 200 z_i samples\n",
        "    x = x.reshape(x.shape[0],200,784)\n",
        "        \n",
        "    logP_xi_zi = -(F.binary_cross_entropy_with_logits(x_, x, reduction='none').sum(dim=2))\n",
        "    \n",
        "  \n",
        "  z = z.reshape(x.shape[0],200,100)\n",
        "  mu = mu.reshape(x.shape[0],200,100)\n",
        "  logvar = logvar.reshape(x.shape[0],200,100)\n",
        "  logP_z__Q_z_x = (0.5*((z-mu)**2/logvar.exp() - z**2).sum(dim=-1) + 0.5*logvar.sum(dim=-1))  #log of two gaussians\n",
        "  \n",
        "  #LogSumExpTrick:\n",
        "  pi_max = torch.max(logP_xi_zi + logP_z__Q_z_x, dim=1,keepdim=True)[0]  #Rescaling with pi_max\n",
        "  \n",
        "  logP_xi = pi_max.view(-1) + torch.log((logP_xi_zi + logP_z__Q_z_x - pi_max).exp().mean(dim=-1))\n",
        "  \n",
        "  return logP_xi\n",
        "\n",
        "  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wgwBpAwqqSlo",
        "colab_type": "code",
        "outputId": "0ca6dcfc-dc1a-4609-d907-77941c380682",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "cell_type": "code",
      "source": [
        "## Performing the calculation with the logP_xi function\n",
        "## on the valid and test set:\n",
        "\n",
        "logP_valid=[]     #stocking (log p(x_1),...,log(px_M)) estimates of size (M,) for valid\n",
        "logP_test = []    #stocking (log p(x_1),...,log(px_M)) estimates of size (M,) for test\n",
        "\n",
        "for batch_idx, x in enumerate(valid):\n",
        "  \n",
        "  x = x.to(device)\n",
        "  logP = logP_xi(clf,x)\n",
        "  logP_valid.append(logP)\n",
        "  if (batch_idx+1) % 15 ==0:\n",
        "    print(batch_idx/157)     # tracking progress\n",
        "  \n",
        "for batch_idx, x in enumerate(test):\n",
        "  \n",
        "  x = x.to(device)\n",
        "  logP = logP_xi(clf,x)\n",
        "  logP_test.append(logP)\n",
        "  if (batch_idx+1) % 15 == 0:\n",
        "    print(batch_idx/157)   # tracking progress\n",
        "  \n",
        "\n",
        "estimate_valid = 0\n",
        "estimate_test = 0\n",
        "   \n",
        "for estimate in logP_valid:          \n",
        "  estimate_valid += estimate.sum()  \n",
        "  \n",
        "for estimate in logP_test:          \n",
        "  estimate_test += estimate.sum()\n",
        "  \n",
        "estimate_valid /= 10000             #Average on the dataset (N = 10 000)\n",
        "estimate_test /= 10000\n",
        "\n",
        "print(estimate_valid,estimate_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.UpsamplingBilinear2d is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.08917197452229299\n",
            "0.18471337579617833\n",
            "0.2802547770700637\n",
            "0.37579617834394907\n",
            "0.4713375796178344\n",
            "0.5668789808917197\n",
            "0.6624203821656051\n",
            "0.7579617834394905\n",
            "0.8535031847133758\n",
            "0.9490445859872612\n",
            "0.08917197452229299\n",
            "0.18471337579617833\n",
            "0.2802547770700637\n",
            "0.37579617834394907\n",
            "0.4713375796178344\n",
            "0.5668789808917197\n",
            "0.6624203821656051\n",
            "0.7579617834394905\n",
            "0.8535031847133758\n",
            "0.9490445859872612\n",
            "tensor(-88.2604, device='cuda:0') tensor(-87.6443, device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jnoKNokgclmR",
        "colab_type": "code",
        "outputId": "881c53a7-962b-4416-d5c1-1902a1ce6c37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "clf.eval()\n",
        "test_loss=0\n",
        "for batch_idx, x in enumerate(test):\n",
        "  x = x.to(device)\n",
        "  x_, mu, logvar = clf(x)\n",
        "  loss = criterion(x_, x, mu, logvar)\n",
        "  test_loss += loss.item()\n",
        "test_batches = batch_idx\n",
        "clf.train()\n",
        "  \n",
        "print('====> ELBO on Test set: {:.4f}'.format(\n",
        "      -1*test_loss/test_batches ))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.UpsamplingBilinear2d is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "====> ELBO on Test set: -93.4580\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PUpEUjAHbiEm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
